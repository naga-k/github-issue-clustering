{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783f8dd4",
   "metadata": {},
   "source": [
    "# Issue Clustering\n",
    "\n",
    "This notebook clusters issue/feedback items from a CSV, evaluates against provided labels, and compares three strategies:\n",
    "\n",
    "- Agglomerative clustering (cosine, threshold-based, batch)\n",
    "- Vector-style incremental assignment (simulates vector DB neighbor-join)\n",
    "- Centroid-style incremental assignment\n",
    "\n",
    "It uses Gemini embeddings by default (set `GOOGLE_API_KEY`) or any SentenceTransformer model if you change `model_name`. Ensure `issues_raw.csv` exists with columns `title`, `body`, and optional `label`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d849d01",
   "metadata": {},
   "source": [
    "Add a `.env` file with `GOOGLE_API_KEY=your_key` if you want Gemini embeddings. To run offline, switch `model_name` to a SentenceTransformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58d3bae",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.1' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/workspaces/soulcaster-text-clustering/.venv/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!python3 -m venv venv && source venv/bin/activate && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19e95b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import sklearn\n",
    "from packaging import version\n",
    "from google import genai\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "genai_client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\")) if os.getenv(\"GOOGLE_API_KEY\") else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8dd950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_issues(\n",
    "    issues,                      # list[dict] with keys: title, body (body optional)\n",
    "    model_name=\"gemini-embedding-001\",\n",
    "    sim_threshold=0.70,          # higher => fewer, tighter clusters\n",
    "    min_cluster_size=2,\n",
    "    truncate_body_chars=1500,    # prevent very long bodies from dominating\n",
    "    label_singletons_as_minus_one=True,\n",
    "):\n",
    "    \"\"\"Cluster issue dicts by semantic similarity.\n",
    "\n",
    "    Embeddings source is chosen by model_name:\n",
    "    - If model_name starts with \"gemini\" (e.g., \"gemini-embedding-001\"), use Gemini via GOOGLE_API_KEY.\n",
    "    - Otherwise, use SentenceTransformer(model_name).\n",
    "\n",
    "    Returns both raw labels from sklearn and display labels (optionally -1 for singletons).\n",
    "    \"\"\"\n",
    "    def _to_text(val):\n",
    "        if val is None:\n",
    "            return \"\"\n",
    "        if isinstance(val, str):\n",
    "            return val\n",
    "        if isinstance(val, float):\n",
    "            return \"\" if np.isnan(val) else str(val)\n",
    "        return str(val)\n",
    "\n",
    "    texts = []\n",
    "    for it in issues:\n",
    "        title = _to_text(it.get(\"title\")).strip()\n",
    "        body = _to_text(it.get(\"body\")).strip()\n",
    "        if truncate_body_chars:\n",
    "            body = body[:truncate_body_chars]\n",
    "        text = f\"{title}\\n\\n{body}\".strip()\n",
    "        if not text:\n",
    "            text = \"[empty]\"\n",
    "        texts.append(text)\n",
    "\n",
    "    if not texts:\n",
    "        return {\n",
    "            \"labels\": np.array([], dtype=int),\n",
    "            \"display_labels\": np.array([], dtype=int),\n",
    "            \"clusters\": {},\n",
    "            \"singletons\": [],\n",
    "            \"texts\": texts,\n",
    "        }\n",
    "\n",
    "    if len(texts) == 1:\n",
    "        labels = np.array([0], dtype=int)\n",
    "        display_labels = np.array([-1], dtype=int) if label_singletons_as_minus_one else labels.copy()\n",
    "        clusters = {0: [0]} if min_cluster_size <= 1 else {}\n",
    "        singletons = [0]\n",
    "        return {\n",
    "            \"labels\": labels,\n",
    "            \"display_labels\": display_labels,\n",
    "            \"clusters\": clusters,\n",
    "            \"singletons\": singletons,\n",
    "            \"texts\": texts,\n",
    "        }\n",
    "\n",
    "    if model_name.lower().startswith(\"gemini\"):\n",
    "        if genai_client is None:\n",
    "            raise RuntimeError(\"GOOGLE_API_KEY not set; populate .env or environment\")\n",
    "        resp = genai_client.models.embed_content(\n",
    "            model=model_name,\n",
    "            contents=texts,\n",
    "            config={\"output_dimensionality\": 768},\n",
    "        )\n",
    "        emb = np.asarray([e.values for e in resp.embeddings], dtype=np.float32)\n",
    "        print(\"Using Gemini Text Embedding\")\n",
    "    else:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        emb = model.encode(\n",
    "            texts,\n",
    "            batch_size=32,\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        emb = np.asarray(emb, dtype=np.float32)\n",
    "        print(\"Using HF Sentence Transformer Embedding\")\n",
    "\n",
    "    norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    emb = emb / norms\n",
    "\n",
    "    dist_threshold = 1.0 - float(sim_threshold)\n",
    "\n",
    "    kwargs = dict(n_clusters=None, linkage=\"average\", distance_threshold=dist_threshold)\n",
    "    if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "        kwargs[\"metric\"] = \"cosine\"\n",
    "    else:\n",
    "        kwargs[\"affinity\"] = \"cosine\"\n",
    "\n",
    "    cl = AgglomerativeClustering(**kwargs)\n",
    "    labels = cl.fit_predict(emb)\n",
    "\n",
    "    clusters = {}\n",
    "    for i, lab in enumerate(labels):\n",
    "        clusters.setdefault(int(lab), []).append(i)\n",
    "\n",
    "    kept = {k: v for k, v in clusters.items() if len(v) >= min_cluster_size}\n",
    "    singletons = [v[0] for k, v in clusters.items() if len(v) == 1]\n",
    "    kept = dict(sorted(kept.items(), key=lambda kv: len(kv[1]), reverse=True))\n",
    "\n",
    "    display_labels = labels.copy()\n",
    "    if label_singletons_as_minus_one:\n",
    "        for i in singletons:\n",
    "            display_labels[i] = -1\n",
    "\n",
    "    return {\n",
    "        \"labels\": labels,\n",
    "        \"display_labels\": display_labels,\n",
    "        \"clusters\": kept,\n",
    "        \"singletons\": singletons,\n",
    "        \"texts\": texts,\n",
    "    }\n",
    "\n",
    "def print_clusters(result, issues, max_items_per_cluster=8):\n",
    "    for cid, idxs in result[\"clusters\"].items():\n",
    "        print(f\"\\n=== Cluster {cid}  (n={len(idxs)}) ===\")\n",
    "        for j in idxs[:max_items_per_cluster]:\n",
    "            t = (issues[j].get(\"title\") or \"\").strip().replace(\"\\n\", \" \")\n",
    "            print(f\"- [{j:02d}] {t[:140]}\")\n",
    "    if result[\"singletons\"]:\n",
    "        print(f\"\\nSingletons (n={len(result['singletons'])}): {result['singletons']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9f654",
   "metadata": {},
   "source": [
    "## Tabular cluster view helper\n",
    "Creates a DataFrame with cluster id, cluster size, issue index, title, and body so you can sort/filter in the notebook UI. Use the raw issues CSV helper if you want to label without clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26e47a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_as_dataframe(result, issues):\n",
    "    \"\"\"Return a DataFrame with raw/display labels, cluster size, issue index, title, body.\"\"\"\n",
    "    rows = []\n",
    "    cluster_sizes = {cid: len(idxs) for cid, idxs in result.get(\"clusters\", {}).items()}\n",
    "    display_labels = result.get(\"display_labels\", result.get(\"labels\", []))\n",
    "    labels = result.get(\"labels\", [])\n",
    "\n",
    "    def _to_text(val):\n",
    "        if val is None:\n",
    "            return \"\"\n",
    "        if isinstance(val, str):\n",
    "            return val\n",
    "        if isinstance(val, float):\n",
    "            return \"\" if np.isnan(val) else str(val)\n",
    "        return str(val)\n",
    "\n",
    "    for j, issue in enumerate(issues):\n",
    "        raw_lab = int(labels[j]) if len(labels) > j else None\n",
    "        disp_lab = int(display_labels[j]) if len(display_labels) > j else None\n",
    "        cluster_size = cluster_sizes.get(raw_lab, 1) if raw_lab is not None else 1\n",
    "        rows.append({\n",
    "            \"cluster_raw\": raw_lab,\n",
    "            \"cluster_display\": disp_lab,\n",
    "            \"is_singleton\": disp_lab == -1,\n",
    "            \"cluster_size\": cluster_size,\n",
    "            \"idx\": j,\n",
    "            \"title\": _to_text(issue.get(\"title\")),\n",
    "            \"body\": _to_text(issue.get(\"body\")),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values([\"is_singleton\", \"cluster_size\", \"cluster_display\", \"idx\"], ascending=[True, False, True, True]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_issues_csv(issues, path=\"issues_raw.csv\"):\n",
    "    \"\"\"Save the raw (unclustered) issues to CSV for manual labeling.\"\"\"\n",
    "    def _to_text(val):\n",
    "        if val is None:\n",
    "            return \"\"\n",
    "        if isinstance(val, str):\n",
    "            return val\n",
    "        if isinstance(val, float):\n",
    "            return \"\" if np.isnan(val) else str(val)\n",
    "        return str(val)\n",
    "\n",
    "    rows = []\n",
    "    for idx, it in enumerate(issues):\n",
    "        rows.append({\n",
    "            \"idx\": idx,\n",
    "            \"title\": _to_text(it.get(\"title\")).strip(),\n",
    "            \"body\": _to_text(it.get(\"body\")).strip(),\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Wrote {len(df)} issues to {path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce218fe7",
   "metadata": {},
   "source": [
    "## Example: cluster from CSV\n",
    "Load labeled issues from `issues_raw.csv`, cluster them, and evaluate against the provided labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61245471",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "GOOGLE_API_KEY not set; populate .env or environment",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m issues = df_src[[\u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m\"\u001b[39m]].to_dict(\u001b[33m\"\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m true_labels = df_src[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df_src.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m res = \u001b[43mcluster_issues\u001b[49m\u001b[43m(\u001b[49m\u001b[43missues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.72\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_cluster_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_singletons_as_minus_one\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m print_clusters(res, issues)\n\u001b[32m     11\u001b[39m df_clusters = clusters_as_dataframe(res, issues)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mcluster_issues\u001b[39m\u001b[34m(issues, model_name, sim_threshold, min_cluster_size, truncate_body_chars, label_singletons_as_minus_one)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name.lower().startswith(\u001b[33m\"\u001b[39m\u001b[33mgemini\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m genai_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_API_KEY not set; populate .env or environment\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m     resp = genai_client.models.embed_content(\n\u001b[32m     63\u001b[39m         model=model_name,\n\u001b[32m     64\u001b[39m         contents=texts,\n\u001b[32m     65\u001b[39m         config={\u001b[33m\"\u001b[39m\u001b[33moutput_dimensionality\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m768\u001b[39m},\n\u001b[32m     66\u001b[39m     )\n\u001b[32m     67\u001b[39m     emb = np.asarray([e.values \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m resp.embeddings], dtype=np.float32)\n",
      "\u001b[31mRuntimeError\u001b[39m: GOOGLE_API_KEY not set; populate .env or environment"
     ]
    }
   ],
   "source": [
    "# Load issues from CSV, cluster, and evaluate\n",
    "csv_path = \"issues_raw.csv\"\n",
    "\n",
    "df_src = pd.read_csv(csv_path)\n",
    "issues = df_src[[\"title\", \"body\"]].to_dict(\"records\")\n",
    "true_labels = df_src[\"label\"].tolist() if \"label\" in df_src.columns else None\n",
    "\n",
    "res = cluster_issues(issues, sim_threshold=0.72, min_cluster_size=2, label_singletons_as_minus_one=False)\n",
    "print_clusters(res, issues)\n",
    "\n",
    "df_clusters = clusters_as_dataframe(res, issues)\n",
    "display(df_clusters.head(200))\n",
    "\n",
    "if true_labels is not None:\n",
    "    # Evaluate clustering vs. provided labels\n",
    "    from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, homogeneity_completeness_v_measure\n",
    "\n",
    "    # Map string labels to ints for metrics\n",
    "    true_enc, _ = pd.factorize(true_labels)\n",
    "    pred_enc, _ = pd.factorize(res[\"display_labels\"])\n",
    "\n",
    "    ari = adjusted_rand_score(true_enc, pred_enc)\n",
    "    ami = adjusted_mutual_info_score(true_enc, pred_enc, average_method=\"arithmetic\")\n",
    "    h, c, v = homogeneity_completeness_v_measure(true_enc, pred_enc)\n",
    "\n",
    "    print(f\"ARI: {ari:.3f}\")\n",
    "    print(f\"AMI: {ami:.3f}\")\n",
    "    print(f\"Homogeneity: {h:.3f}  Completeness: {c:.3f}  V-Measure: {v:.3f}\")\n",
    "else:\n",
    "    print(\"No labels column found; skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "620e75a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "GOOGLE_API_KEY not set; populate .env or environment",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m true_labels = df_src[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df_src.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     97\u001b[39m texts = _prep_texts(issues)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m emb = \u001b[43membed_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-embedding-001\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Agglomerative (existing)\u001b[39;00m\n\u001b[32m    101\u001b[39m res_aggl = cluster_issues(issues, sim_threshold=\u001b[32m0.72\u001b[39m, min_cluster_size=\u001b[32m2\u001b[39m, label_singletons_as_minus_one=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36membed_texts\u001b[39m\u001b[34m(texts, model_name)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name.lower().startswith(\u001b[33m\"\u001b[39m\u001b[33mgemini\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m genai_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_API_KEY not set; populate .env or environment\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m     resp = genai_client.models.embed_content(\n\u001b[32m     29\u001b[39m         model=model_name,\n\u001b[32m     30\u001b[39m         contents=texts,\n\u001b[32m     31\u001b[39m         config={\u001b[33m\"\u001b[39m\u001b[33moutput_dimensionality\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m768\u001b[39m},\n\u001b[32m     32\u001b[39m     )\n\u001b[32m     33\u001b[39m     emb = np.asarray([e.values \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m resp.embeddings], dtype=np.float32)\n",
      "\u001b[31mRuntimeError\u001b[39m: GOOGLE_API_KEY not set; populate .env or environment"
     ]
    }
   ],
   "source": [
    "# Compare agglomerative vs vector-like vs centroid-like on CSV labels\n",
    "import math\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, homogeneity_completeness_v_measure\n",
    "\n",
    "def _prep_texts(issues, truncate_body_chars=1500):\n",
    "    def _to_text(val):\n",
    "        if val is None:\n",
    "            return \"\"\n",
    "        if isinstance(val, str):\n",
    "            return val\n",
    "        if isinstance(val, float):\n",
    "            return \"\" if np.isnan(val) else str(val)\n",
    "        return str(val)\n",
    "    texts = []\n",
    "    for it in issues:\n",
    "        title = _to_text(it.get(\"title\")).strip()\n",
    "        body = _to_text(it.get(\"body\")).strip()\n",
    "        if truncate_body_chars:\n",
    "            body = body[:truncate_body_chars]\n",
    "        txt = f\"{title}\\n\\n{body}\".strip()\n",
    "        texts.append(txt or \"[empty]\")\n",
    "    return texts\n",
    "\n",
    "def embed_texts(texts, model_name=\"gemini-embedding-001\"):\n",
    "    if model_name.lower().startswith(\"gemini\"):\n",
    "        if genai_client is None:\n",
    "            raise RuntimeError(\"GOOGLE_API_KEY not set; populate .env or environment\")\n",
    "        resp = genai_client.models.embed_content(\n",
    "            model=model_name,\n",
    "            contents=texts,\n",
    "            config={\"output_dimensionality\": 768},\n",
    "        )\n",
    "        emb = np.asarray([e.values for e in resp.embeddings], dtype=np.float32)\n",
    "    else:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        emb = model.encode(\n",
    "            texts,\n",
    "            batch_size=32,\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        emb = np.asarray(emb, dtype=np.float32)\n",
    "    norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    return emb / norms\n",
    "\n",
    "def cosine(a, b):\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "def vector_like_cluster(embeddings, threshold=0.72):\n",
    "    labels = []\n",
    "    next_cluster = 0\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        sims = [cosine(emb, embeddings[j]) for j in range(i)]\n",
    "        similars = [j for j, s in enumerate(sims) if s >= threshold]\n",
    "        if similars:\n",
    "            labels.append(labels[similars[0]])\n",
    "        else:\n",
    "            labels.append(next_cluster)\n",
    "            next_cluster += 1\n",
    "    return np.array(labels, dtype=int)\n",
    "\n",
    "def centroid_cluster(embeddings, threshold=0.65):\n",
    "    centroids = []\n",
    "    labels = []\n",
    "    for emb in embeddings:\n",
    "        if not centroids:\n",
    "            labels.append(0)\n",
    "            centroids.append(emb.copy())\n",
    "            continue\n",
    "        sims = [cosine(emb, c) for c in centroids]\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        if sims[best_idx] >= threshold:\n",
    "            k = best_idx\n",
    "            count_k = labels.count(k)\n",
    "            centroids[k] = (centroids[k] * count_k + emb) / (count_k + 1)\n",
    "            labels.append(k)\n",
    "        else:\n",
    "            labels.append(len(centroids))\n",
    "            centroids.append(emb.copy())\n",
    "    return np.array(labels, dtype=int)\n",
    "\n",
    "def evaluate(true_labels, pred_labels, name):\n",
    "    true_enc, _ = pd.factorize(pd.Series(true_labels))\n",
    "    pred_enc, _ = pd.factorize(pd.Series(pred_labels))\n",
    "    ari = adjusted_rand_score(true_enc, pred_enc)\n",
    "    ami = adjusted_mutual_info_score(true_enc, pred_enc, average_method=\"arithmetic\")\n",
    "    h, c, v = homogeneity_completeness_v_measure(true_enc, pred_enc)\n",
    "    print(f\"{name} -> ARI: {ari:.3f} | AMI: {ami:.3f} | H: {h:.3f} C: {c:.3f} V: {v:.3f}\")\n",
    "\n",
    "# Load data\n",
    "csv_path = \"issues_raw.csv\"\n",
    "df_src = pd.read_csv(csv_path)\n",
    "issues = df_src[[\"title\", \"body\"]].to_dict(\"records\")\n",
    "true_labels = df_src[\"label\"].tolist() if \"label\" in df_src.columns else None\n",
    "\n",
    "texts = _prep_texts(issues)\n",
    "emb = embed_texts(texts, model_name=\"gemini-embedding-001\")\n",
    "\n",
    "# Agglomerative (existing)\n",
    "res_aggl = cluster_issues(issues, sim_threshold=0.72, min_cluster_size=2, label_singletons_as_minus_one=False)\n",
    "\n",
    "# Vector-style (incremental neighbor above threshold)\n",
    "vec_labels = vector_like_cluster(emb, threshold=0.72)\n",
    "\n",
    "# Centroid-style (assign to best centroid above threshold; else new)\n",
    "cent_labels = centroid_cluster(emb, threshold=0.65)\n",
    "\n",
    "if true_labels is not None:\n",
    "    evaluate(true_labels, res_aggl[\"display_labels\"], \"Agglomerative\")\n",
    "    evaluate(true_labels, vec_labels, \"Vector-like\")\n",
    "    evaluate(true_labels, cent_labels, \"Centroid-like\")\n",
    "else:\n",
    "    print(\"No labels column found; skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d555eb50",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "GOOGLE_API_KEY not set; populate .env or environment",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m true_labels = df_src[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df_src.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     97\u001b[39m texts = _prep_texts(issues)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m emb = \u001b[43membed_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-embedding-001\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Agglomerative (existing)\u001b[39;00m\n\u001b[32m    101\u001b[39m res_aggl = cluster_issues(issues, sim_threshold=\u001b[32m0.72\u001b[39m, min_cluster_size=\u001b[32m2\u001b[39m, label_singletons_as_minus_one=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36membed_texts\u001b[39m\u001b[34m(texts, model_name)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name.lower().startswith(\u001b[33m\"\u001b[39m\u001b[33mgemini\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m genai_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_API_KEY not set; populate .env or environment\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m     resp = genai_client.models.embed_content(\n\u001b[32m     29\u001b[39m         model=model_name,\n\u001b[32m     30\u001b[39m         contents=texts,\n\u001b[32m     31\u001b[39m         config={\u001b[33m\"\u001b[39m\u001b[33moutput_dimensionality\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m768\u001b[39m},\n\u001b[32m     32\u001b[39m     )\n\u001b[32m     33\u001b[39m     emb = np.asarray([e.values \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m resp.embeddings], dtype=np.float32)\n",
      "\u001b[31mRuntimeError\u001b[39m: GOOGLE_API_KEY not set; populate .env or environment"
     ]
    }
   ],
   "source": [
    "# Compare agglomerative vs vector-like vs centroid-like on CSV labels\n",
    "import math\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, homogeneity_completeness_v_measure\n",
    "\n",
    "def _prep_texts(issues, truncate_body_chars=1500):\n",
    "    def _to_text(val):\n",
    "        if val is None:\n",
    "            return \"\"\n",
    "        if isinstance(val, str):\n",
    "            return val\n",
    "        if isinstance(val, float):\n",
    "            return \"\" if np.isnan(val) else str(val)\n",
    "        return str(val)\n",
    "    texts = []\n",
    "    for it in issues:\n",
    "        title = _to_text(it.get(\"title\")).strip()\n",
    "        body = _to_text(it.get(\"body\")).strip()\n",
    "        if truncate_body_chars:\n",
    "            body = body[:truncate_body_chars]\n",
    "        txt = f\"{title}\\n\\n{body}\".strip()\n",
    "        texts.append(txt or \"[empty]\")\n",
    "    return texts\n",
    "\n",
    "def embed_texts(texts, model_name=\"gemini-embedding-001\"):\n",
    "    if model_name.lower().startswith(\"gemini\"):\n",
    "        if genai_client is None:\n",
    "            raise RuntimeError(\"GOOGLE_API_KEY not set; populate .env or environment\")\n",
    "        resp = genai_client.models.embed_content(\n",
    "            model=model_name,\n",
    "            contents=texts,\n",
    "            config={\"output_dimensionality\": 768},\n",
    "        )\n",
    "        emb = np.asarray([e.values for e in resp.embeddings], dtype=np.float32)\n",
    "    else:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        emb = model.encode(\n",
    "            texts,\n",
    "            batch_size=32,\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        emb = np.asarray(emb, dtype=np.float32)\n",
    "    norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    return emb / norms\n",
    "\n",
    "def cosine(a, b):\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "def vector_like_cluster(embeddings, threshold=0.72):\n",
    "    labels = []\n",
    "    next_cluster = 0\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        sims = [cosine(emb, embeddings[j]) for j in range(i)]\n",
    "        similars = [j for j, s in enumerate(sims) if s >= threshold]\n",
    "        if similars:\n",
    "            labels.append(labels[similars[0]])\n",
    "        else:\n",
    "            labels.append(next_cluster)\n",
    "            next_cluster += 1\n",
    "    return np.array(labels, dtype=int)\n",
    "\n",
    "def centroid_cluster(embeddings, threshold=0.65):\n",
    "    centroids = []\n",
    "    labels = []\n",
    "    for emb in embeddings:\n",
    "        if not centroids:\n",
    "            labels.append(0)\n",
    "            centroids.append(emb.copy())\n",
    "            continue\n",
    "        sims = [cosine(emb, c) for c in centroids]\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        if sims[best_idx] >= threshold:\n",
    "            k = best_idx\n",
    "            count_k = labels.count(k)\n",
    "            centroids[k] = (centroids[k] * count_k + emb) / (count_k + 1)\n",
    "            labels.append(k)\n",
    "        else:\n",
    "            labels.append(len(centroids))\n",
    "            centroids.append(emb.copy())\n",
    "    return np.array(labels, dtype=int)\n",
    "\n",
    "def evaluate(true_labels, pred_labels, name):\n",
    "    true_enc, _ = pd.factorize(pd.Series(true_labels))\n",
    "    pred_enc, _ = pd.factorize(pd.Series(pred_labels))\n",
    "    ari = adjusted_rand_score(true_enc, pred_enc)\n",
    "    ami = adjusted_mutual_info_score(true_enc, pred_enc, average_method=\"arithmetic\")\n",
    "    h, c, v = homogeneity_completeness_v_measure(true_enc, pred_enc)\n",
    "    print(f\"{name} -> ARI: {ari:.3f} | AMI: {ami:.3f} | H: {h:.3f} C: {c:.3f} V: {v:.3f}\")\n",
    "\n",
    "# Load data\n",
    "csv_path = \"issues_raw.csv\"\n",
    "df_src = pd.read_csv(csv_path)\n",
    "issues = df_src[[\"title\", \"body\"]].to_dict(\"records\")\n",
    "true_labels = df_src[\"label\"].tolist() if \"label\" in df_src.columns else None\n",
    "\n",
    "texts = _prep_texts(issues)\n",
    "emb = embed_texts(texts, model_name=\"gemini-embedding-001\")\n",
    "\n",
    "# Agglomerative (existing)\n",
    "res_aggl = cluster_issues(issues, sim_threshold=0.72, min_cluster_size=2, label_singletons_as_minus_one=False)\n",
    "\n",
    "# Vector-style (incremental neighbor above threshold)\n",
    "vec_labels = vector_like_cluster(emb, threshold=0.72)\n",
    "\n",
    "# Centroid-style (assign to best centroid above threshold; else new)\n",
    "cent_labels = centroid_cluster(emb, threshold=0.65)\n",
    "\n",
    "if true_labels is not None:\n",
    "    evaluate(true_labels, res_aggl[\"display_labels\"], \"Agglomerative\")\n",
    "    evaluate(true_labels, vec_labels, \"Vector-like\")\n",
    "    evaluate(true_labels, cent_labels, \"Centroid-like\")\n",
    "else:\n",
    "    print(\"No labels column found; skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a0e5c4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Vector sweeps\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m vt \u001b[38;5;129;01min\u001b[39;00m vector_thresholds:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     vec_labels = vector_like_cluster(\u001b[43memb\u001b[49m, threshold=vt)\n\u001b[32m     18\u001b[39m     pred_enc, _ = pd.factorize(pd.Series(vec_labels))\n\u001b[32m     19\u001b[39m     ari = adjusted_rand_score(true_enc, pred_enc)\n",
      "\u001b[31mNameError\u001b[39m: name 'emb' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameter sweep for vector/centroid thresholds\n",
    "from itertools import product\n",
    "\n",
    "vector_thresholds = [0.68, 0.70, 0.72, 0.74, 0.76, 0.78]\n",
    "centroid_thresholds = [0.60, 0.63, 0.66, 0.69, 0.72, 0.75]\n",
    "agg_thresholds = [0.68, 0.70, 0.72, 0.74, 0.76, 0.78]\n",
    "\n",
    "results = []\n",
    "true_labels = df_src[\"label\"].tolist() if \"label\" in df_src.columns else None\n",
    "if true_labels is None:\n",
    "    print(\"No labels; skipping sweep.\")\n",
    "else:\n",
    "    true_enc, _ = pd.factorize(pd.Series(true_labels))\n",
    "\n",
    "    # Vector sweeps\n",
    "    for vt in vector_thresholds:\n",
    "        vec_labels = vector_like_cluster(emb, threshold=vt)\n",
    "        pred_enc, _ = pd.factorize(pd.Series(vec_labels))\n",
    "        ari = adjusted_rand_score(true_enc, pred_enc)\n",
    "        ami = adjusted_mutual_info_score(true_enc, pred_enc, average_method=\"arithmetic\")\n",
    "        h, c, v = homogeneity_completeness_v_measure(true_enc, pred_enc)\n",
    "        results.append({\"method\": \"vector\", \"threshold\": vt, \"ari\": ari, \"ami\": ami, \"h\": h, \"c\": c, \"v\": v})\n",
    "\n",
    "    # Centroid sweeps\n",
    "    for ct in centroid_thresholds:\n",
    "        cent_labels = centroid_cluster(emb, threshold=ct)\n",
    "        pred_enc, _ = pd.factorize(pd.Series(cent_labels))\n",
    "        ari = adjusted_rand_score(true_enc, pred_enc)\n",
    "        ami = adjusted_mutual_info_score(true_enc, pred_enc, average_method=\"arithmetic\")\n",
    "        h, c, v = homogeneity_completeness_v_measure(true_enc, pred_enc)\n",
    "        results.append({\"method\": \"centroid\", \"threshold\": ct, \"ari\": ari, \"ami\": ami, \"h\": h, \"c\": c, \"v\": v})\n",
    "\n",
    "    # Agglomerative sweeps\n",
    "    for th in agg_thresholds:\n",
    "        dist_threshold = 1.0 - float(th)\n",
    "        kwargs = dict(n_clusters=None, linkage=\"average\", distance_threshold=dist_threshold)\n",
    "        if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "            kwargs[\"metric\"] = \"cosine\"\n",
    "        else:\n",
    "            kwargs[\"affinity\"] = \"cosine\"\n",
    "        cl = AgglomerativeClustering(**kwargs)\n",
    "        labels = cl.fit_predict(emb)\n",
    "        pred_enc, _ = pd.factorize(pd.Series(labels))\n",
    "        ari = adjusted_rand_score(true_enc, pred_enc)\n",
    "        ami = adjusted_mutual_info_score(true_enc, pred_enc, average_method=\"arithmetic\")\n",
    "        h, c, v = homogeneity_completeness_v_measure(true_enc, pred_enc)\n",
    "        results.append({\"method\": \"agglomerative\", \"threshold\": th, \"ari\": ari, \"ami\": ami, \"h\": h, \"c\": c, \"v\": v})\n",
    "\n",
    "    df_sweep = pd.DataFrame(results).sort_values([\"method\", \"ari\"], ascending=[True, False]).reset_index(drop=True)\n",
    "    display(df_sweep)\n",
    "\n",
    "    # Show best per method\n",
    "    best = df_sweep.sort_values([\"method\", \"ari\"], ascending=[True, False]).groupby(\"method\").head(1)\n",
    "    print(\"Best per method (by ARI):\")\n",
    "    display(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nr17sfj2dto",
   "metadata": {},
   "source": [
    "## Vector Visualization\n",
    "\n",
    "Visualize the high-dimensional embeddings in 2D/3D space using UMAP dimensionality reduction. This helps understand how vectors cluster and whether similar items are actually close together in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9atm1fgz6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def visualize_embeddings_2d(embeddings, true_labels=None, pred_labels=None, texts=None, titles=None):\n",
    "    \"\"\"\n",
    "    Reduce embeddings to 2D using UMAP and create interactive visualizations.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: numpy array of shape (n_samples, n_features)\n",
    "        true_labels: ground truth labels (optional)\n",
    "        pred_labels: predicted cluster labels (optional)\n",
    "        texts: list of text content for hover display (optional)\n",
    "        titles: list of titles for each point (optional)\n",
    "    \"\"\"\n",
    "    print(\"Reducing dimensions with UMAP (2D)...\")\n",
    "    reducer_2d = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "    embedding_2d = reducer_2d.fit_transform(embeddings)\n",
    "    \n",
    "    # Prepare hover text\n",
    "    hover_texts = []\n",
    "    for i in range(len(embeddings)):\n",
    "        hover = f\"Index: {i}<br>\"\n",
    "        if titles is not None and i < len(titles):\n",
    "            title = titles[i][:100]  # truncate long titles\n",
    "            hover += f\"Title: {title}<br>\"\n",
    "        if true_labels is not None:\n",
    "            hover += f\"True Label: {true_labels[i]}<br>\"\n",
    "        if pred_labels is not None:\n",
    "            hover += f\"Predicted: {pred_labels[i]}<br>\"\n",
    "        hover_texts.append(hover)\n",
    "    \n",
    "    # Create figure(s)\n",
    "    if true_labels is not None and pred_labels is not None:\n",
    "        # Side-by-side comparison\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=(\"Ground Truth Labels\", \"Predicted Clusters\"),\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # Ground truth plot\n",
    "        for label in sorted(set(true_labels)):\n",
    "            mask = [tl == label for tl in true_labels]\n",
    "            indices = [i for i, m in enumerate(mask) if m]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=embedding_2d[indices, 0],\n",
    "                    y=embedding_2d[indices, 1],\n",
    "                    mode='markers',\n",
    "                    name=f'True {label}',\n",
    "                    text=[hover_texts[i] for i in indices],\n",
    "                    hovertemplate='%{text}<extra></extra>',\n",
    "                    marker=dict(size=8, opacity=0.7),\n",
    "                    legendgroup='true'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Predicted clusters plot\n",
    "        for label in sorted(set(pred_labels)):\n",
    "            mask = [pl == label for pl in pred_labels]\n",
    "            indices = [i for i, m in enumerate(mask) if m]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=embedding_2d[indices, 0],\n",
    "                    y=embedding_2d[indices, 1],\n",
    "                    mode='markers',\n",
    "                    name=f'Pred {label}',\n",
    "                    text=[hover_texts[i] for i in indices],\n",
    "                    hovertemplate='%{text}<extra></extra>',\n",
    "                    marker=dict(size=8, opacity=0.7),\n",
    "                    legendgroup='pred'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_xaxes(title_text=\"UMAP Dimension 1\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"UMAP Dimension 1\", row=1, col=2)\n",
    "        fig.update_yaxes(title_text=\"UMAP Dimension 2\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"UMAP Dimension 2\", row=1, col=2)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=600,\n",
    "            width=1400,\n",
    "            title_text=\"2D Vector Space Visualization\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "    elif pred_labels is not None:\n",
    "        # Just predicted labels\n",
    "        df_plot = pd.DataFrame({\n",
    "            'x': embedding_2d[:, 0],\n",
    "            'y': embedding_2d[:, 1],\n",
    "            'cluster': [str(l) for l in pred_labels],\n",
    "            'hover': hover_texts\n",
    "        })\n",
    "        \n",
    "        fig = px.scatter(\n",
    "            df_plot, x='x', y='y', color='cluster',\n",
    "            hover_data={'hover': True, 'x': False, 'y': False, 'cluster': False},\n",
    "            title='2D Vector Space - Predicted Clusters',\n",
    "            labels={'x': 'UMAP Dimension 1', 'y': 'UMAP Dimension 2'}\n",
    "        )\n",
    "        fig.update_traces(marker=dict(size=8, opacity=0.7))\n",
    "        fig.update_layout(height=600, width=900)\n",
    "        \n",
    "    elif true_labels is not None:\n",
    "        # Just ground truth\n",
    "        df_plot = pd.DataFrame({\n",
    "            'x': embedding_2d[:, 0],\n",
    "            'y': embedding_2d[:, 1],\n",
    "            'label': [str(l) for l in true_labels],\n",
    "            'hover': hover_texts\n",
    "        })\n",
    "        \n",
    "        fig = px.scatter(\n",
    "            df_plot, x='x', y='y', color='label',\n",
    "            hover_data={'hover': True, 'x': False, 'y': False, 'label': False},\n",
    "            title='2D Vector Space - Ground Truth Labels',\n",
    "            labels={'x': 'UMAP Dimension 1', 'y': 'UMAP Dimension 2'}\n",
    "        )\n",
    "        fig.update_traces(marker=dict(size=8, opacity=0.7))\n",
    "        fig.update_layout(height=600, width=900)\n",
    "    else:\n",
    "        # No labels - just show points\n",
    "        df_plot = pd.DataFrame({\n",
    "            'x': embedding_2d[:, 0],\n",
    "            'y': embedding_2d[:, 1],\n",
    "            'hover': hover_texts\n",
    "        })\n",
    "        \n",
    "        fig = px.scatter(\n",
    "            df_plot, x='x', y='y',\n",
    "            hover_data={'hover': True, 'x': False, 'y': False},\n",
    "            title='2D Vector Space',\n",
    "            labels={'x': 'UMAP Dimension 1', 'y': 'UMAP Dimension 2'}\n",
    "        )\n",
    "        fig.update_traces(marker=dict(size=8, opacity=0.7))\n",
    "        fig.update_layout(height=600, width=900)\n",
    "    \n",
    "    fig.show()\n",
    "    return embedding_2d\n",
    "\n",
    "\n",
    "def visualize_embeddings_3d(embeddings, true_labels=None, pred_labels=None, texts=None, titles=None):\n",
    "    \"\"\"\n",
    "    Reduce embeddings to 3D using UMAP and create interactive 3D visualization.\n",
    "    \"\"\"\n",
    "    print(\"Reducing dimensions with UMAP (3D)...\")\n",
    "    reducer_3d = umap.UMAP(n_components=3, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "    embedding_3d = reducer_3d.fit_transform(embeddings)\n",
    "    \n",
    "    # Prepare hover text\n",
    "    hover_texts = []\n",
    "    for i in range(len(embeddings)):\n",
    "        hover = f\"Index: {i}<br>\"\n",
    "        if titles is not None and i < len(titles):\n",
    "            title = titles[i][:100]\n",
    "            hover += f\"Title: {title}<br>\"\n",
    "        if true_labels is not None:\n",
    "            hover += f\"True Label: {true_labels[i]}<br>\"\n",
    "        if pred_labels is not None:\n",
    "            hover += f\"Predicted: {pred_labels[i]}<br>\"\n",
    "        hover_texts.append(hover)\n",
    "    \n",
    "    # Use predicted labels if available, otherwise true labels\n",
    "    labels_to_plot = pred_labels if pred_labels is not None else true_labels\n",
    "    label_name = \"Predicted Cluster\" if pred_labels is not None else \"Ground Truth Label\"\n",
    "    \n",
    "    if labels_to_plot is not None:\n",
    "        df_plot = pd.DataFrame({\n",
    "            'x': embedding_3d[:, 0],\n",
    "            'y': embedding_3d[:, 1],\n",
    "            'z': embedding_3d[:, 2],\n",
    "            'label': [str(l) for l in labels_to_plot],\n",
    "            'hover': hover_texts\n",
    "        })\n",
    "        \n",
    "        fig = px.scatter_3d(\n",
    "            df_plot, x='x', y='y', z='z', color='label',\n",
    "            hover_data={'hover': True, 'x': False, 'y': False, 'z': False, 'label': False},\n",
    "            title=f'3D Vector Space - {label_name}',\n",
    "            labels={'x': 'UMAP Dim 1', 'y': 'UMAP Dim 2', 'z': 'UMAP Dim 3'}\n",
    "        )\n",
    "    else:\n",
    "        df_plot = pd.DataFrame({\n",
    "            'x': embedding_3d[:, 0],\n",
    "            'y': embedding_3d[:, 1],\n",
    "            'z': embedding_3d[:, 2],\n",
    "            'hover': hover_texts\n",
    "        })\n",
    "        \n",
    "        fig = px.scatter_3d(\n",
    "            df_plot, x='x', y='y', z='z',\n",
    "            hover_data={'hover': True, 'x': False, 'y': False, 'z': False},\n",
    "            title='3D Vector Space',\n",
    "            labels={'x': 'UMAP Dim 1', 'y': 'UMAP Dim 2', 'z': 'UMAP Dim 3'}\n",
    "        )\n",
    "    \n",
    "    fig.update_traces(marker=dict(size=5, opacity=0.7))\n",
    "    fig.update_layout(height=700, width=900)\n",
    "    fig.show()\n",
    "    return embedding_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2697hsjq5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the embeddings in 2D space\n",
    "# This shows ground truth labels vs predicted clusters side-by-side\n",
    "\n",
    "titles = [issue.get(\"title\", \"\") for issue in issues]\n",
    "embedding_2d = visualize_embeddings_2d(\n",
    "    emb, \n",
    "    true_labels=true_labels,\n",
    "    pred_labels=res_aggl[\"display_labels\"],\n",
    "    titles=titles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zqolta4owc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize in 3D - interactive, can rotate and zoom\n",
    "# Shows predicted clusters by default\n",
    "\n",
    "embedding_3d = visualize_embeddings_3d(\n",
    "    emb,\n",
    "    true_labels=true_labels,\n",
    "    pred_labels=res_aggl[\"display_labels\"],\n",
    "    titles=titles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52t42ymr3bw",
   "metadata": {},
   "source": [
    "## Vector Similarity Explorer\n",
    "\n",
    "Explore which items are most similar to each other in vector space. This helps understand what the embeddings consider \"similar\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7vna1soyhbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(embeddings, query_idx, top_k=10, issues=None):\n",
    "    \"\"\"\n",
    "    Find the most similar items to a given query item.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: numpy array of embeddings\n",
    "        query_idx: index of the query item\n",
    "        top_k: number of most similar items to return\n",
    "        issues: list of issue dicts with 'title' and 'body' (optional)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with similarity scores and content\n",
    "    \"\"\"\n",
    "    query_emb = embeddings[query_idx]\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    similarities = np.dot(embeddings, query_emb)\n",
    "    \n",
    "    # Get top-k most similar (excluding the query itself)\n",
    "    similar_indices = np.argsort(similarities)[::-1]\n",
    "    similar_indices = [i for i in similar_indices if i != query_idx][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in similar_indices:\n",
    "        row = {\n",
    "            'idx': idx,\n",
    "            'similarity': float(similarities[idx]),\n",
    "        }\n",
    "        if issues is not None:\n",
    "            row['title'] = issues[idx].get('title', '')[:100]\n",
    "            row['body'] = issues[idx].get('body', '')[:200]\n",
    "        results.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Show query item\n",
    "    print(f\"Query item [{query_idx}]:\")\n",
    "    if issues is not None:\n",
    "        print(f\"  Title: {issues[query_idx].get('title', '')[:100]}\")\n",
    "        print(f\"  Body: {issues[query_idx].get('body', '')[:200]}\")\n",
    "    print(f\"\\nMost similar items:\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def visualize_similarity_heatmap(embeddings, indices=None, labels=None, max_items=50):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing pairwise similarities between items.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: numpy array of embeddings\n",
    "        indices: specific indices to visualize (optional, defaults to first max_items)\n",
    "        labels: labels for each item (optional)\n",
    "        max_items: maximum number of items to show (for performance)\n",
    "    \"\"\"\n",
    "    if indices is None:\n",
    "        indices = list(range(min(len(embeddings), max_items)))\n",
    "    \n",
    "    # Compute similarity matrix for selected indices\n",
    "    selected_embs = embeddings[indices]\n",
    "    similarity_matrix = np.dot(selected_embs, selected_embs.T)\n",
    "    \n",
    "    # Create labels for axes\n",
    "    if labels is not None:\n",
    "        tick_labels = [f\"{i} (L{labels[i]})\" for i in indices]\n",
    "    else:\n",
    "        tick_labels = [str(i) for i in indices]\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=similarity_matrix,\n",
    "        x=tick_labels,\n",
    "        y=tick_labels,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0.7,  # center the colorscale at typical similarity threshold\n",
    "        colorbar=dict(title=\"Cosine Similarity\")\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Pairwise Similarity Heatmap ({len(indices)} items)',\n",
    "        xaxis_title='Item Index',\n",
    "        yaxis_title='Item Index',\n",
    "        width=800,\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3roj8pp1gc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find items most similar to item 0\n",
    "df_similar = find_most_similar(emb, query_idx=0, top_k=10, issues=issues)\n",
    "display(df_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3gns1oweej1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pairwise similarities as a heatmap\n",
    "# This shows which items are similar to each other\n",
    "# Darker red = more similar, darker blue = less similar\n",
    "\n",
    "sim_matrix = visualize_similarity_heatmap(\n",
    "    emb, \n",
    "    indices=None,  # Uses first 50 items by default\n",
    "    labels=true_labels,\n",
    "    max_items=50\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
